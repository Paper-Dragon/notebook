import{ah as i,ai as n,ar as e,ak as s,al as r,am as l}from"./app-CPG_wNR7.js";const o="/assets/image-20250602105341874-dUdCwCdO.png",p="/assets/image-20250602210703011-Cf-nj8t9.png",d="/assets/image-20250605154713465-tKiXL1q5.png",c="/assets/image-20250605154947330-yGe2tUz3.png",h="/assets/image-20250605155119077-BvAUFYqK.png",m="/assets/image-20250605165446256-1Y6v4E_r.png",g="/assets/image-20250605174915036-O5oD_JM6.png",u="/assets/image-20250605175024107-CesS9KPX.png",b="/assets/image-20250606134328883-CPf_4e2c.png",f="/assets/image-20250606134647414-Dv1Ugdfj.png",_="/assets/image-20250606134942824-CTr-X8Lo.png",x="/assets/image-20250606135355386-DZxxH3Ly.png",A={};function P(T,a){const t=r("Mermaid");return l(),n("div",null,[a[0]||(a[0]=e('<h1 id="大模型技术基础" tabindex="-1"><a class="header-anchor" href="#大模型技术基础"><span>大模型技术基础</span></a></h1><h2 id="大语言模型概念" tabindex="-1"><a class="header-anchor" href="#大语言模型概念"><span>大语言模型概念</span></a></h2><p>定义： 通常指具有超大规模参数的预训练模型</p><p>架构：主要为transformer解码器架构</p><p>训练：</p><ul><li>预训练（base model) 建立模型的<strong>基础能力</strong><ul><li>数据： 海量文本数据</li><li>优化：预测下一个词</li></ul></li><li>后训练 (instruct model) 增强模型的<strong>任务能力</strong><ul><li>数据： 大量指令数据</li><li>优化： SFT、RL等方法</li></ul></li><li>下游应用 <ul><li>测速（推理）</li></ul></li></ul>',6)),s(t,{code:"eJxTVa3OzMsssVKoVkrLyS9PzkgsKlEC8TJKcnN8EpNSc4qB3LTEnOLU2lqFWlVVLrgyBZ8gLgUgSEosTo3PzU9JzYlWStDSermo5cW6tc93N2tpgaWfTd3wrHfd+z2znm3d/rK9/9m09mdz1kAEwfJP9sx42jMNKA/U+Gxr95MdQNTwZMeqF+t7E5RiwUoy84pLikqTS5BseTqhD7stCk+XLAdZ09P+ZPcSbNYoBLuFPG5oDPJ5vrbz2bSdzzZPhVmTk1hcEl9QlJ9elJgbrQR0yLMdO57umvJ8ygqoAoRPbW3BHDBbwdbWDs2JWFwNVAURKE4uUYCaYYdiJRcAMU6xMw=="}),a[1]||(a[1]=e('<h2 id="训练阶段对比" tabindex="-1"><a class="header-anchor" href="#训练阶段对比"><span>训练阶段对比</span></a></h2><table><thead><tr><th>对比方面</th><th>预训练 (Pre-training)</th><th>后训练 (Post-training)</th></tr></thead><tbody><tr><td>核心目标</td><td>建立模型基础能力</td><td>将基座模型适配到具体应用场景</td></tr><tr><td>数据资源</td><td>数万亿词元的自然语言文本</td><td>数十万、数百万到数千万指令数据</td></tr><tr><td>所需算力</td><td>耗费百卡、千卡甚至万卡算力数月时间 <em>（大致估计）</em></td><td>耗费数十卡、数百卡数天到数十天时间 <em>（大致估计）</em></td></tr><tr><td>使用方式</td><td>通常为few-shot提示</td><td>可以直接进行zero-shot使用</td></tr></tbody></table><p><em>此部分算力估计为一个大致估计，需要根据模型大小、数据数量、训练框架等多方面因素确定</em></p><h2 id="大语言模型构建概览" tabindex="-1"><a class="header-anchor" href="#大语言模型构建概览"><span>大语言模型构建概览</span></a></h2><p>大语言模型预训练（Pre-training)</p><ul><li>使用与下游任务无关的大规模数据进行模型参数的初始训练 <ul><li>基于Transformer解码器架构，进行下一个词预测</li><li>数据数量、数据质量都比较关键</li></ul></li></ul><p>大语言模型后训练（Post-training)</p><ul><li>指令微调（Instruction Tuning)v 【有人也叫SFT】 <ul><li>使用输入与输出配对的指令数据对于模型进行微调</li><li>提升模型通过问答形式进行任务求解的能力</li></ul></li></ul><p><img src="'+o+'" alt="image-20250602105341874"></p><ul><li>人类对齐（Human Alignment） <ul><li>将大语言模型与人类的期望、需求以及价值对齐。</li><li>基于人类反馈的强化学习对齐（RLHF）。</li></ul></li></ul><p><img src="'+p+'" alt="image-20250602210703011"></p><h2 id="扩展定律" tabindex="-1"><a class="header-anchor" href="#扩展定律"><span>扩展定律</span></a></h2><h3 id="什么是扩展定律" tabindex="-1"><a class="header-anchor" href="#什么是扩展定律"><span>什么是扩展定律</span></a></h3><ul><li>通过扩展参数规模、数据规模和扩大算力，大语言模型的能力会出现显著提升</li><li>扩展定律再本次大模型浪潮中起到了重要作用</li></ul><p><img src="'+d+'" alt="image-20250605154713465"></p><h3 id="km扩展定律" tabindex="-1"><a class="header-anchor" href="#km扩展定律"><span>KM扩展定律</span></a></h3><ul><li>OpenAI团队建立了神经语言模型性能与参数模型（N）、数据规模（D）和计算算力（C）之间的幂律关系</li></ul><p><img src="'+c+'" alt="image-20250605154947330"></p><h3 id="chinchilla扩展定律" tabindex="-1"><a class="header-anchor" href="#chinchilla扩展定律"><span>Chinchilla扩展定律</span></a></h3><p>DeepMind团队于2022年提出另一种形式的扩展定律，旨在指导大语言模型充分利用给定的酸锂资源优化</p><p><img src="'+h+'" alt="image-20250605155119077"></p><h3 id="深入讨论" tabindex="-1"><a class="header-anchor" href="#深入讨论"><span>深入讨论</span></a></h3><p>模型的语言建模损失可以进行下述分解</p><p>$L(x)=\\underbrace{L_{\\infty}}<em>{\\text {不可约损失 }}+\\underbrace{\\left(\\frac{x</em>{0}}{x}\\right)^{\\alpha_{x}}}_{\\text {可约损失 }}$</p><p>可约损失： 真实分布和模型分布之间KL散度，可通过优化减少</p><p>不可约损失：真实数据分布的熵，无法通过优化减少</p><h4 id="扩展定律可能存在边际效益递减" tabindex="-1"><a class="header-anchor" href="#扩展定律可能存在边际效益递减"><span>扩展定律可能存在边际效益递减</span></a></h4><ul><li>随着模型参数、数据数量的扩展，模型性能的增益将逐渐减小</li><li>目前开发数据已经接近枯竭，难以支持扩展定律的持续支持</li></ul><h4 id="可预测的扩展-predictable-scaling" tabindex="-1"><a class="header-anchor" href="#可预测的扩展-predictable-scaling"><span>可预测的扩展（Predictable Scaling）</span></a></h4><ul><li>使用小模型性能去预估大模型性能，或帮助超参数选择</li><li>训练过程中使用模型早期性能来预估后续性能</li></ul><p><img src="'+m+'" alt="image-20250605165446256"></p><h2 id="涌现能力" tabindex="-1"><a class="header-anchor" href="#涌现能力"><span>涌现能力</span></a></h2><p>什么是涌现能力？</p><ul><li>原始论文定义： 在小模型中不存在、但在大模型中出现的能力</li><li>模型扩展到一定规模时，特定任务性能突然出现显著跃升趋势，远超随机水平</li></ul><p><img src="'+g+'" alt="image-20250605174915036"></p><h3 id="涌现能力可以部分归因于评测设置" tabindex="-1"><a class="header-anchor" href="#涌现能力可以部分归因于评测设置"><span>涌现能力可以部分归因于评测设置</span></a></h3><p>本教程定义其为“代表性能力”，并不区分是否在小模型中存在</p><p><img src="'+u+'" alt="image-20250605175024107"></p><h3 id="代表性能力" tabindex="-1"><a class="header-anchor" href="#代表性能力"><span>代表性能力</span></a></h3><h4 id="指令遵循-instruction-following" tabindex="-1"><a class="header-anchor" href="#指令遵循-instruction-following"><span>指令遵循（Instruction Following）</span></a></h4><ul><li>大语言模型能够按照自然语言指令来执行对应的任务</li></ul><p><img src="'+b+'" alt="image-20250606134328883"></p><h4 id="上下文学习-in-context-learning" tabindex="-1"><a class="header-anchor" href="#上下文学习-in-context-learning"><span>上下文学习（In-context Learning)</span></a></h4><ul><li>在提示中为语言模型提供自然语言指令和任务示例，无需显式梯度更新就成为测试样本生成预期输出。</li></ul><p><img src="'+f+'" alt="image-20250606134647414"></p><h4 id="逐步推理-step-by-step-resoning" tabindex="-1"><a class="header-anchor" href="#逐步推理-step-by-step-resoning"><span>逐步推理（Step-by-step Resoning)</span></a></h4><p>在提示中引入任务相关的中间推理步骤来加强复杂任务的求解，从而获得更可靠的答案</p><p><img src="'+_+'" alt="image-20250606134942824"></p><h2 id="涌现能力与扩展定律的关系" tabindex="-1"><a class="header-anchor" href="#涌现能力与扩展定律的关系"><span>涌现能力与扩展定律的关系</span></a></h2><p>涌现能力和阔扎定律是两种描述规模效应的度量方法</p><p><img src="'+x+'" alt="image-20250606135355386"></p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><p>大模型核心技术</p><ul><li>规模扩展：扩展定律奠定了早期大模型的技术路线，产生了巨大的性能提升。</li><li>数据工程：数据数量、数据质量以及配置方法极其关键</li><li>高效预训练：需要建立可预测、可扩展的大规模训练架构</li><li>能力激发：预训练后可以通过微调、对其、提示工程等技术进行能力激活</li><li>人类对其：需要设计对齐技术减少模型使用风险，并进一步提升模型性能</li><li>工具使用：使用外部加强模型的弱点，拓展其neng&#39;li</li></ul>',54))])}const E=i(A,[["render",P]]),D=JSON.parse('{"path":"/note-book/AI-Training/01%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.html","title":"大模型技术基础","lang":"zh-CN","frontmatter":{"description":"大模型技术基础 大语言模型概念 定义： 通常指具有超大规模参数的预训练模型 架构：主要为transformer解码器架构 训练： 预训练（base model) 建立模型的基础能力 数据： 海量文本数据 优化：预测下一个词 后训练 (instruct model) 增强模型的任务能力 数据： 大量指令数据 优化： SFT、RL等方法 下游应用 测速（推...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"大模型技术基础\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-06-18T01:52:45.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Paper-Dragon\\",\\"url\\":\\"https://github.com/Paper-Dragon\\",\\"email\\":\\"2678885646@qq.com\\"}]}"],["meta",{"property":"og:url","content":"https://www.geekery.cn/note-book/AI-Training/01%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.html"}],["meta",{"property":"og:site_name","content":"运维开发绿皮书"}],["meta",{"property":"og:title","content":"大模型技术基础"}],["meta",{"property":"og:description","content":"大模型技术基础 大语言模型概念 定义： 通常指具有超大规模参数的预训练模型 架构：主要为transformer解码器架构 训练： 预训练（base model) 建立模型的基础能力 数据： 海量文本数据 优化：预测下一个词 后训练 (instruct model) 增强模型的任务能力 数据： 大量指令数据 优化： SFT、RL等方法 下游应用 测速（推..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-06-18T01:52:45.000Z"}],["meta",{"property":"article:modified_time","content":"2025-06-18T01:52:45.000Z"}]]},"git":{"createdTime":1749107714000,"updatedTime":1750211565000,"contributors":[{"name":"Paper-Dragon","username":"Paper-Dragon","email":"2678885646@qq.com","commits":3,"url":"https://github.com/Paper-Dragon"},{"name":"PaperDragon-Bot","username":"PaperDragon-Bot","email":"2678885646@qq.com","commits":1,"url":"https://github.com/PaperDragon-Bot"}],"changelog":[{"hash":"4ec93fe31108bcd91455fde51905a2d7d33b611f","time":1750211565000,"email":"2678885646@qq.com","author":"Paper-Dragon","message":"排序 大语言模型"},{"hash":"65b321470c7ffe693d666bda7950defaa09a6160","time":1749176428000,"email":"2678885646@qq.com","author":"PaperDragon-Bot","message":"扩展法则、涌现能力"},{"hash":"49725dc0390830318722ab7ea235a34982435796","time":1749119486000,"email":"2678885646@qq.com","author":"Paper-Dragon","message":"1.2"},{"hash":"86d660f039b3d10949983498c054557f363f2345","time":1749107714000,"email":"2678885646@qq.com","author":"Paper-Dragon","message":"src/note-book/AI-Training/大语言模型基础.md"}]},"readingTime":{"minutes":4.89,"words":1468},"filePathRelative":"note-book/AI-Training/01大语言模型基础.md","excerpt":"\\n<h2>大语言模型概念</h2>\\n<p>定义： 通常指具有超大规模参数的预训练模型</p>\\n<p>架构：主要为transformer解码器架构</p>\\n<p>训练：</p>\\n<ul>\\n<li>预训练（base model)  建立模型的<strong>基础能力</strong>\\n<ul>\\n<li>数据： 海量文本数据</li>\\n<li>优化：预测下一个词</li>\\n</ul>\\n</li>\\n<li>后训练 (instruct model) 增强模型的<strong>任务能力</strong>\\n<ul>\\n<li>数据： 大量指令数据</li>\\n<li>优化： SFT、RL等方法</li>\\n</ul>\\n</li>\\n<li>下游应用\\n<ul>\\n<li>测速（推理）</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{E as comp,D as data};
