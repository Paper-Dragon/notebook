import{a,c as e,g as r,o as s}from"./app-DJZTso3N.js";const n="/assets/image-20250709173129303-DdHUtPxB.png",o="/assets/image-20250709173320576-DjdUPYrw.png",i="/assets/image-20250709173524409-CTERb6Rq.png",p="/assets/image-20250709173703607-CBJxc6LM.png",d="/assets/image-20250709174313357-Cr-p51jE.png",l="/assets/image-20250709174411105-CfR6_r_p.png",h="/assets/image-20250709174458173-BGmUjuIB.png",g="/assets/image-20250709174645134-BkkrJKLx.png",m="/assets/image-20250709174245261-Dv52rNie.png",c="/assets/image-20250709174655763-BwGGjA4h.png",_="/assets/image-20250709174843648-Br7PJC-Q.png",u="/assets/image-20250709175101374-YmvcyxVe.png",x={};function b(f,t){return s(),e("div",null,[...t[0]||(t[0]=[r('<h1 id="模型详解配置" tabindex="-1"><a class="header-anchor" href="#模型详解配置"><span>模型详解配置</span></a></h1><p>构建大模型需要考虑的因素归一化方法、位置编码、激活函数、注意力计算</p><p><img src="'+n+'" alt="image-20250709173129303"></p><p>层数L、注意力头数N、特征维度N</p><h2 id="归一化方法" tabindex="-1"><a class="header-anchor" href="#归一化方法"><span>归一化方法</span></a></h2><h4 id="为什么要做归一化" tabindex="-1"><a class="header-anchor" href="#为什么要做归一化"><span>为什么要做归一化?</span></a></h4><ul><li><p><strong>不同特征在空间中的尺度不同，对损失优化的影响不一致</strong><br> 特征尺度差异会导致损失函数各方向的梯度下降速度不同。尺度大的特征梯度更新剧烈，迫使模型花费更多迭代次数调整其他特征权重，降低优化效率。</p></li><li><p><strong>提升训练稳定性，加速模型收敛</strong><br> 归一化使所有特征处于相近的数值范围（如[0,1]或[-1,1]）。这使优化路径更平滑，梯度更新方向更稳定，减少震荡风险，从而加快模型收敛速度。</p></li></ul><p><img src="'+o+'" alt="image-20250709173320576"></p><h3 id="方法" tabindex="-1"><a class="header-anchor" href="#方法"><span>方法</span></a></h3><p><img src="'+i+'" alt="image-20250709173524409"></p><h3 id="归一化模块位置" tabindex="-1"><a class="header-anchor" href="#归一化模块位置"><span>归一化模块位置</span></a></h3><p><img src="'+p+'" alt="image-20250709173703607"></p><h4 id="层后归一化-post-layer-normalization-post-norm" tabindex="-1"><a class="header-anchor" href="#层后归一化-post-layer-normalization-post-norm"><span>层后归一化(Post-Layer Normalization, Post-Norm)</span></a></h4><p><strong>定义</strong> （归一化模块放置于残差计算之后）</p><p><strong>优点</strong></p><ul><li>加快训练收敛速度</li><li>防止梯度爆炸或梯度消失</li><li>降低神经网络对于超参数的敏感性</li></ul><p><strong>缺点</strong></p><ul><li>可能导致训练不稳定</li><li>目前较少单独使用</li></ul><h4 id="层前归一化-pre-layer-normalization-pre-norm" tabindex="-1"><a class="header-anchor" href="#层前归一化-pre-layer-normalization-pre-norm"><span>层前归一化(Pre-Layer Normalization, Pre-Norm)</span></a></h4><p><strong>定义</strong> （归一化模块应用在每个子层之前）</p><p><strong>优点</strong></p><ul><li>训练更加稳定</li><li>主流模型采用较多</li></ul><p><strong>缺点</strong></p><ul><li>性能略有逊色</li></ul><h4 id="夹心归一化-sandwich-norm" tabindex="-1"><a class="header-anchor" href="#夹心归一化-sandwich-norm"><span>夹心归一化(Sandwich-Norm)</span></a></h4><p><strong>定义</strong> （通过双重归一化叠加：在子层<em>前</em>和子层<em>后</em>各加一次归一化）</p><p><strong>核心特性</strong></p><ul><li>Pre-Norm与Post-Norm的复合结构</li><li>归一化逻辑：输入 → <strong>首次归一化</strong> → Sublayer计算 → <strong>二次归一化</strong> → 残差连接</li></ul><p><strong>优势</strong></p><ul><li>理论上融合Pre-Norm的稳定性与Post-Norm的性能增益</li><li>梯度调节能力更强（双重归一化约束）</li></ul><p><strong>局限性</strong></p><ul><li>计算开销显著增加（额外归一化层）</li><li>仍存在训练震荡风险</li><li>实际应用较少，多见于理论研究</li></ul><h2 id="位置编码" tabindex="-1"><a class="header-anchor" href="#位置编码"><span>位置编码</span></a></h2><p><img src="'+d+'" alt="image-20250709174313357"></p><p>绝对位置编码代表性的是旋转位置编码</p><h3 id="绝对位置编码-旋转位置编码" tabindex="-1"><a class="header-anchor" href="#绝对位置编码-旋转位置编码"><span>绝对位置编码：旋转位置编码</span></a></h3><p><img src="'+l+'" alt="image-20250709174411105"></p><p><img src="'+h+'" alt="image-20250709174458173"></p><h3 id="相对位置编码-alibi位置编码" tabindex="-1"><a class="header-anchor" href="#相对位置编码-alibi位置编码"><span>相对位置编码：ALiBi位置编码</span></a></h3><p><img src="'+g+'" alt="image-20250709174645134"></p><h2 id="激活函数" tabindex="-1"><a class="header-anchor" href="#激活函数"><span>激活函数</span></a></h2><p><strong><img src="'+m+'" alt="image-20250709174245261"></strong></p><h2 id="注意力计算" tabindex="-1"><a class="header-anchor" href="#注意力计算"><span>注意力计算</span></a></h2><p><img src="'+c+'" alt="image-20250709174655763"></p><h3 id="对硬件优化" tabindex="-1"><a class="header-anchor" href="#对硬件优化"><span>对硬件优化</span></a></h3><p><img src="'+_+'" alt="image-20250709174843648"></p><h2 id="moe模型" tabindex="-1"><a class="header-anchor" href="#moe模型"><span>MOE模型</span></a></h2><p>举例： deepseek、Mixtral</p><p>目的是在不显著提升计算成本的同时实现对于模型参数的扩展。</p><p><img src="'+u+'" alt="image-20250709175101374"></p><h3 id="混合专家架构-mixture-of-experts-moe" tabindex="-1"><a class="header-anchor" href="#混合专家架构-mixture-of-experts-moe"><span>混合专家架构(Mixture-of-Experts, MoE)</span></a></h3><p><strong>核心定义</strong><br> $$ \\text{MoELayer}(x_t) = \\sum_{i=1}^{K} G(x_t)_i \\cdot E_i(x_t)<br> $$ 其中路由函数：<br> $$ G(x_t) = \\text{softmax}(\\text{topk}(x_t \\cdot W_G))<br> $$</p><p><strong>基本组成</strong></p><ol><li><p><strong>专家组件</strong></p><ul><li>$K$个独立的前馈神经网络 ${E_1, E_2, ..., E_K}$</li><li>每个专家具备相同网络结构，参数不同</li></ul></li><li><p><strong>路由网络</strong></p><ul><li>$W_G$：权重矩阵（将输入词元$x_t$映射为$K$维得分向量）</li><li>$\\text{topk}$：选择得分最高的$k$个专家（通常$k \\ll K$）</li><li>$\\text{softmax}$：归一化获得激活权重（未选中专家权重置$0$）</li></ul></li></ol><p><strong>运行流程</strong><br><code>输入词元 → 路由计算→ 筛选topk专家→ 专家并行计算→ 加权输出求和</code></p><p><strong>核心优势</strong></p><ul><li><strong>稀疏激活</strong>：仅计算部分专家输出（节省计算资源）</li><li><strong>超参扩容</strong>：通过增加专家数$K$提升模型容量（计算量仅由$k$决定）</li><li><strong>动态适配</strong>：根据输入特性自动分配处理专家</li></ul><p><strong>显著局限</strong></p><ul><li>路由决策不可导（需直通估计器技巧）</li><li>专家负载不均衡风险（需引入辅助损失）</li><li>通信开销大（分布式训练时专家间数据交换）</li></ul><h2 id="常用" tabindex="-1"><a class="header-anchor" href="#常用"><span>常用</span></a></h2><table><thead><tr><th>模型</th><th>混合专家</th><th>归一化</th><th>位置编码</th><th>激活函数</th><th>注意力机制</th><th>层数</th><th>隐藏层维度</th><th>注意力头个数</th><th>头维度</th></tr></thead><tbody><tr><td>LLaMA-3.1 (405B)</td><td>N/A</td><td>Pre RMSNorm</td><td>旋转位置编码</td><td>SwiGLU</td><td>多头隐式注意力</td><td>126</td><td>16,384</td><td>128</td><td>128</td></tr><tr><td>DeepSeek (67B)</td><td>N/A</td><td>Pre RMSNorm</td><td>旋转位置编码</td><td>SwiGLU</td><td>多头隐式注意力</td><td>95</td><td>8,192</td><td>64</td><td>128</td></tr><tr><td>DeepSeek-V2 (236B)</td><td>162 Experts</td><td>Pre RMSNorm</td><td>旋转位置编码</td><td>SwiGLU</td><td>分组查询注意力</td><td>60</td><td>5,120</td><td>40</td><td>128</td></tr><tr><td>DeepSeek-V3 (671B)</td><td>257 Experts</td><td>Pre RMSNorm</td><td>旋转位置编码</td><td>SwiGLU</td><td>分组查询注意力</td><td>61</td><td>7,168</td><td>56</td><td>128</td></tr></tbody></table>',61)])])}const E=a(x,[["render",b]]),N=JSON.parse('{"path":"/note-book/AI-Training/04%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E9%85%8D%E7%BD%AE.html","title":"模型详解配置","lang":"zh-CN","frontmatter":{"description":"模型详解配置 构建大模型需要考虑的因素归一化方法、位置编码、激活函数、注意力计算 image-20250709173129303 层数L、注意力头数N、特征维度N 归一化方法 为什么要做归一化? 不同特征在空间中的尺度不同，对损失优化的影响不一致 特征尺度差异会导致损失函数各方向的梯度下降速度不同。尺度大的特征梯度更新剧烈，迫使模型花费更多迭代次数调整...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"模型详解配置\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-07-09T09:56:47.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Paper-Dragon\\",\\"url\\":\\"https://github.com/Paper-Dragon\\",\\"email\\":\\"2678885646@qq.com\\"}]}"],["meta",{"property":"og:url","content":"https://www.geekery.cn/note-book/AI-Training/04%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E9%85%8D%E7%BD%AE.html"}],["meta",{"property":"og:site_name","content":"运维开发绿皮书"}],["meta",{"property":"og:title","content":"模型详解配置"}],["meta",{"property":"og:description","content":"模型详解配置 构建大模型需要考虑的因素归一化方法、位置编码、激活函数、注意力计算 image-20250709173129303 层数L、注意力头数N、特征维度N 归一化方法 为什么要做归一化? 不同特征在空间中的尺度不同，对损失优化的影响不一致 特征尺度差异会导致损失函数各方向的梯度下降速度不同。尺度大的特征梯度更新剧烈，迫使模型花费更多迭代次数调整..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-09T09:56:47.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-09T09:56:47.000Z"}]]},"git":{"createdTime":1752055007000,"updatedTime":1752055007000,"contributors":[{"name":"Paper-Dragon","username":"Paper-Dragon","email":"2678885646@qq.com","commits":1,"url":"https://github.com/Paper-Dragon"}],"changelog":[{"hash":"99189fda270fd2a45615a2b815f524feee63ddd4","time":1752055007000,"email":"2678885646@qq.com","author":"Paper-Dragon","message":"04模型详解配置.md"}]},"readingTime":{"minutes":3.88,"words":1165},"filePathRelative":"note-book/AI-Training/04模型详解配置.md","excerpt":"\\n<p>构建大模型需要考虑的因素归一化方法、位置编码、激活函数、注意力计算</p>\\n<p></p>\\n<p>层数L、注意力头数N、特征维度N</p>\\n<h2>归一化方法</h2>\\n<h4>为什么要做归一化?</h4>\\n<ul>\\n<li>\\n<p><strong>不同特征在空间中的尺度不同，对损失优化的影响不一致</strong><br>\\n特征尺度差异会导致损失函数各方向的梯度下降速度不同。尺度大的特征梯度更新剧烈，迫使模型花费更多迭代次数调整其他特征权重，降低优化效率。</p>\\n</li>\\n<li>\\n<p><strong>提升训练稳定性，加速模型收敛</strong><br>\\n归一化使所有特征处于相近的数值范围（如[0,1]或[-1,1]）。这使优化路径更平滑，梯度更新方向更稳定，减少震荡风险，从而加快模型收敛速度。</p>\\n</li>\\n</ul>","autoDesc":true}');export{E as comp,N as data};
