import{ah as e,ai as t,ar as n,am as r}from"./app-DLLpMjhl.js";const i="/assets/image-20250617180226663-B5n14wPB.png",s="/assets/image-20250617180330064-CfeGwcr7.png",o="/assets/image-20250617180404154-i08G9Jg6.png",p="/assets/image-20250617180516947-DNDl78-a.png",m="/assets/image-20250617181405802-idj25kjW.png",l="/assets/image-20250617181958884-BjQr0XVl.png",h="/assets/image-20250617182115104-D9g6sNwO.png",c={};function g(d,a){return r(),t("div",null,a[0]||(a[0]=[n('<h1 id="模型架构-transformer模型" tabindex="-1"><a class="header-anchor" href="#模型架构-transformer模型"><span>模型架构-Transformer模型</span></a></h1><h2 id="why-attention" tabindex="-1"><a class="header-anchor" href="#why-attention"><span>Why Attention?</span></a></h2><h3 id="搜索场景" tabindex="-1"><a class="header-anchor" href="#搜索场景"><span>搜索场景</span></a></h3><p>在搜索场景中，人们的目光往往会更加关注左上角的三角区域（即第一条搜索结果的位置)</p><p><img src="'+i+'" alt="image-20250617180226663"></p><h3 id="阅读中" tabindex="-1"><a class="header-anchor" href="#阅读中"><span>阅读中</span></a></h3><p><img src="'+s+'" alt="image-20250617180330064"></p><h3 id="浏览图文信息" tabindex="-1"><a class="header-anchor" href="#浏览图文信息"><span>浏览图文信息</span></a></h3><p><img src="'+o+'" alt="image-20250617180404154"></p><h2 id="注意力机制" tabindex="-1"><a class="header-anchor" href="#注意力机制"><span>注意力机制</span></a></h2><p>注意力机制，可以视为一致基于相似度的查表</p><p><img src="'+p+'" alt="image-20250617180516947"></p><p>核心模块： 注意力</p><ul><li>Transformer完全抛弃传统的CNN和RNN，整个网络结构完全由注意力机制组成</li></ul><h2 id="编码器-解码器结构" tabindex="-1"><a class="header-anchor" href="#编码器-解码器结构"><span>编码器-解码器结构</span></a></h2><ul><li>编码器将输入序列变换为隐藏层特征</li><li>解码器将隐藏层特征变换为输出序列</li></ul><p><img src="'+m+'" alt="image-20250617181405802"></p><h3 id="编码器-将输入变换为隐藏层特征" tabindex="-1"><a class="header-anchor" href="#编码器-将输入变换为隐藏层特征"><span>编码器：将输入变换为隐藏层特征</span></a></h3><ul><li>N个堆叠的编码器层 <ul><li>多头注意力</li><li>前馈网络</li><li>残差连接和层归一化</li></ul></li></ul><p><img src="'+l+'" alt="image-20250617181958884"></p><h3 id="解码器-将隐藏层特征变换为自然语言序列" tabindex="-1"><a class="header-anchor" href="#解码器-将隐藏层特征变换为自然语言序列"><span>解码器：将隐藏层特征变换为自然语言序列</span></a></h3><ul><li>N个堆叠的解码器层 <ul><li>（掩码）多头注意力</li><li>前馈网络</li><li>残差链接和层归一化</li></ul></li></ul><p><img src="'+h+'" alt="image-20250617182115104"></p>',23)]))}const u=e(c,[["render",g]]),_=JSON.parse('{"path":"/note-book/AI-Training/03%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84-Transformer%E6%A8%A1%E5%9E%8B.html","title":"模型架构-Transformer模型","lang":"zh-CN","frontmatter":{"description":"模型架构-Transformer模型 Why Attention? 搜索场景 在搜索场景中，人们的目光往往会更加关注左上角的三角区域（即第一条搜索结果的位置) image-20250617180226663 阅读中 image-20250617180330064 浏览图文信息 image-20250617180404154 注意力机制 注意力机制，可以...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"模型架构-Transformer模型\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-06-18T01:52:45.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Paper-Dragon\\",\\"url\\":\\"https://github.com/Paper-Dragon\\",\\"email\\":\\"2678885646@qq.com\\"}]}"],["meta",{"property":"og:url","content":"https://www.geekery.cn/note-book/AI-Training/03%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84-Transformer%E6%A8%A1%E5%9E%8B.html"}],["meta",{"property":"og:site_name","content":"运维开发绿皮书"}],["meta",{"property":"og:title","content":"模型架构-Transformer模型"}],["meta",{"property":"og:description","content":"模型架构-Transformer模型 Why Attention? 搜索场景 在搜索场景中，人们的目光往往会更加关注左上角的三角区域（即第一条搜索结果的位置) image-20250617180226663 阅读中 image-20250617180330064 浏览图文信息 image-20250617180404154 注意力机制 注意力机制，可以..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-06-18T01:52:45.000Z"}],["meta",{"property":"article:modified_time","content":"2025-06-18T01:52:45.000Z"}]]},"git":{"createdTime":1750211565000,"updatedTime":1750211565000,"contributors":[{"name":"Paper-Dragon","username":"Paper-Dragon","email":"2678885646@qq.com","commits":1,"url":"https://github.com/Paper-Dragon"}],"changelog":[{"hash":"4ec93fe31108bcd91455fde51905a2d7d33b611f","time":1750211565000,"email":"2678885646@qq.com","author":"Paper-Dragon","message":"排序 大语言模型"}]},"readingTime":{"minutes":1.1,"words":329},"filePathRelative":"note-book/AI-Training/03模型架构-Transformer模型.md","excerpt":"\\n<h2>Why Attention?</h2>\\n<h3>搜索场景</h3>\\n<p>在搜索场景中，人们的目光往往会更加关注左上角的三角区域（即第一条搜索结果的位置)</p>\\n<p></p>\\n<h3>阅读中</h3>\\n<p></p>\\n<h3>浏览图文信息</h3>\\n<p></p>\\n<h2>注意力机制</h2>\\n<p>注意力机制，可以视为一致基于相似度的查表</p>\\n<p></p>\\n<p>核心模块： 注意力</p>\\n<ul>\\n<li>Transformer完全抛弃传统的CNN和RNN，整个网络结构完全由注意力机制组成</li>\\n</ul>\\n<h2>编码器-解码器结构</h2>\\n<ul>\\n<li>编码器将输入序列变换为隐藏层特征</li>\\n<li>解码器将隐藏层特征变换为输出序列</li>\\n</ul>","autoDesc":true}');export{u as comp,_ as data};
