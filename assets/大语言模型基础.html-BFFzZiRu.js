import{ah as i,ai as n,ar as e,ak as r,al as l,am as s}from"./app-Dv5AG7hI.js";const o="/assets/image-20250602105341874-dUdCwCdO.png",p="/assets/image-20250602210703011-Cf-nj8t9.png",d="/assets/image-20250605154713465-tKiXL1q5.png",m="/assets/image-20250605154947330-yGe2tUz3.png",c="/assets/image-20250605155119077-BvAUFYqK.png",h="/assets/image-20250605165446256-1Y6v4E_r.png",g="/assets/image-20250605174915036-O5oD_JM6.png",u="/assets/image-20250605175024107-CesS9KPX.png",b={};function f(_,a){const t=l("Mermaid");return s(),n("div",null,[a[0]||(a[0]=e('<h1 id="大模型技术基础" tabindex="-1"><a class="header-anchor" href="#大模型技术基础"><span>大模型技术基础</span></a></h1><h2 id="大语言模型概念" tabindex="-1"><a class="header-anchor" href="#大语言模型概念"><span>大语言模型概念</span></a></h2><p>定义： 通常指具有超大规模参数的预训练模型</p><p>架构：主要为transformer解码器架构</p><p>训练：</p><ul><li>预训练（base model) 建立模型的<strong>基础能力</strong><ul><li>数据： 海量文本数据</li><li>优化：预测下一个词</li></ul></li><li>后训练 (instruct model) 增强模型的<strong>任务能力</strong><ul><li>数据： 大量指令数据</li><li>优化： SFT、RL等方法</li></ul></li><li>下游应用 <ul><li>测速（推理）</li></ul></li></ul>',6)),r(t,{id:"mermaid-63",code:"eJxTVa3OzMsssVKoVkrLyS9PzkgsKlEC8TJKcnN8EpNSc4qB3LTEnOLU2lqFWlVVLrgyBZ8gLgUgSEosTo3PzU9JzYlWStDSermo5cW6tc93N2tpgaWfTd3wrHfd+z2znm3d/rK9/9m09mdz1kAEwfJP9sx42jMNKA/U+Gxr95MdQNTwZMeqF+t7E5RiwUoy84pLikqTS5BseTqhD7stCk+XLAdZ09P+ZPcSbNYoBLuFPG5oDPJ5vrbz2bSdzzZPhVmTk1hcEl9QlJ9elJgbrQR0yLMdO57umvJ8ygqoAoRPbW3BHDBbwdbWDs2JWFwNVAURKE4uUYCaYYdiJRcAMU6xMw=="}),a[1]||(a[1]=e('<h2 id="训练阶段对比" tabindex="-1"><a class="header-anchor" href="#训练阶段对比"><span>训练阶段对比</span></a></h2><table><thead><tr><th>对比方面</th><th>预训练 (Pre-training)</th><th>后训练 (Post-training)</th></tr></thead><tbody><tr><td>核心目标</td><td>建立模型基础能力</td><td>将基座模型适配到具体应用场景</td></tr><tr><td>数据资源</td><td>数万亿词元的自然语言文本</td><td>数十万、数百万到数千万指令数据</td></tr><tr><td>所需算力</td><td>耗费百卡、千卡甚至万卡算力数月时间 <em>（大致估计）</em></td><td>耗费数十卡、数百卡数天到数十天时间 <em>（大致估计）</em></td></tr><tr><td>使用方式</td><td>通常为few-shot提示</td><td>可以直接进行zero-shot使用</td></tr></tbody></table><p><em>此部分算力估计为一个大致估计，需要根据模型大小、数据数量、训练框架等多方面因素确定</em></p><h2 id="大语言模型构建概览" tabindex="-1"><a class="header-anchor" href="#大语言模型构建概览"><span>大语言模型构建概览</span></a></h2><p>大语言模型预训练（Pre-training)</p><ul><li>使用与下游任务无关的大规模数据进行模型参数的初始训练 <ul><li>基于Transformer解码器架构，进行下一个词预测</li><li>数据数量、数据质量都比较关键</li></ul></li></ul><p>大语言模型后训练（Post-training)</p><ul><li>指令微调（Instruction Tuning)v 【有人也叫SFT】 <ul><li>使用输入与输出配对的指令数据对于模型进行微调</li><li>提升模型通过问答形式进行任务求解的能力</li></ul></li></ul><p><img src="'+o+'" alt="image-20250602105341874"></p><ul><li>人类对齐（Human Alignment） <ul><li>将大语言模型与人类的期望、需求以及价值对齐。</li><li>基于人类反馈的强化学习对齐（RLHF）。</li></ul></li></ul><p><img src="'+p+'" alt="image-20250602210703011"></p><h2 id="扩展定律" tabindex="-1"><a class="header-anchor" href="#扩展定律"><span>扩展定律</span></a></h2><h3 id="什么是扩展定律" tabindex="-1"><a class="header-anchor" href="#什么是扩展定律"><span>什么是扩展定律</span></a></h3><ul><li>通过扩展参数规模、数据规模和扩大算力，大语言模型的能力会出现显著提升</li><li>扩展定律再本次大模型浪潮中起到了重要作用</li></ul><p><img src="'+d+'" alt="image-20250605154713465"></p><h3 id="km扩展定律" tabindex="-1"><a class="header-anchor" href="#km扩展定律"><span>KM扩展定律</span></a></h3><ul><li>OpenAI团队建立了神经语言模型性能与参数模型（N）、数据规模（D）和计算算力（C）之间的幂律关系</li></ul><p><img src="'+m+'" alt="image-20250605154947330"></p><h3 id="chinchilla扩展定律" tabindex="-1"><a class="header-anchor" href="#chinchilla扩展定律"><span>Chinchilla扩展定律</span></a></h3><p>DeepMind团队于2022年提出另一种形式的扩展定律，旨在指导大语言模型充分利用给定的酸锂资源优化</p><p><img src="'+c+'" alt="image-20250605155119077"></p><h3 id="深入讨论" tabindex="-1"><a class="header-anchor" href="#深入讨论"><span>深入讨论</span></a></h3><p>模型的语言建模损失可以进行下述分解</p><p>$L(x)=\\underbrace{L_{\\infty}}<em>{\\text {不可约损失 }}+\\underbrace{\\left(\\frac{x</em>{0}}{x}\\right)^{\\alpha_{x}}}_{\\text {可约损失 }}$</p><p>可约损失： 真实分布和模型分布之间KL散度，可通过优化减少</p><p>不可约损失：真实数据分布的熵，无法通过优化减少</p><h4 id="扩展定律可能存在边际效益递减" tabindex="-1"><a class="header-anchor" href="#扩展定律可能存在边际效益递减"><span>扩展定律可能存在边际效益递减</span></a></h4><ul><li>随着模型参数、数据数量的扩展，模型性能的增益将逐渐减小</li><li>目前开发数据已经接近枯竭，难以支持扩展定律的持续支持</li></ul><h4 id="可预测的扩展-predictable-scaling" tabindex="-1"><a class="header-anchor" href="#可预测的扩展-predictable-scaling"><span>可预测的扩展（Predictable Scaling）</span></a></h4><ul><li>使用小模型性能去预估大模型性能，或帮助超参数选择</li><li>训练过程中使用模型早期性能来预估后续性能</li></ul><p><img src="'+h+'" alt="image-20250605165446256"></p><h2 id="涌现能力" tabindex="-1"><a class="header-anchor" href="#涌现能力"><span>涌现能力</span></a></h2><p>什么是涌现能力？</p><ul><li>原始论文定义： 在小模型中不存在、但在大模型中出现的能力</li><li>模型扩展到一定规模时，特定任务性能突然出现显著跃升趋势，远超随机水平</li></ul><p><img src="'+g+'" alt="image-20250605174915036"></p><h3 id="涌现能力可以部分归因于评测设置" tabindex="-1"><a class="header-anchor" href="#涌现能力可以部分归因于评测设置"><span>涌现能力可以部分归因于评测设置</span></a></h3><p>本教程定义其为“代表性能力”，并不区分是否在小模型中存在</p><p><img src="'+u+'" alt="image-20250605175024107"></p>',38))])}const x=i(b,[["render",f]]),E=JSON.parse('{"path":"/note-book/AI-Training/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.html","title":"大模型技术基础","lang":"zh-CN","frontmatter":{"description":"大模型技术基础 大语言模型概念 定义： 通常指具有超大规模参数的预训练模型 架构：主要为transformer解码器架构 训练： 预训练（base model) 建立模型的基础能力 数据： 海量文本数据 优化：预测下一个词 后训练 (instruct model) 增强模型的任务能力 数据： 大量指令数据 优化： SFT、RL等方法 下游应用 测速（推...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"大模型技术基础\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-06-05T10:31:26.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Paper-Dragon\\",\\"url\\":\\"https://github.com/Paper-Dragon\\",\\"email\\":\\"2678885646@qq.com\\"}]}"],["meta",{"property":"og:url","content":"https://www.geekery.cn/note-book/AI-Training/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.html"}],["meta",{"property":"og:site_name","content":"运维开发绿皮书"}],["meta",{"property":"og:title","content":"大模型技术基础"}],["meta",{"property":"og:description","content":"大模型技术基础 大语言模型概念 定义： 通常指具有超大规模参数的预训练模型 架构：主要为transformer解码器架构 训练： 预训练（base model) 建立模型的基础能力 数据： 海量文本数据 优化：预测下一个词 后训练 (instruct model) 增强模型的任务能力 数据： 大量指令数据 优化： SFT、RL等方法 下游应用 测速（推..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-06-05T10:31:26.000Z"}],["meta",{"property":"article:modified_time","content":"2025-06-05T10:31:26.000Z"}]]},"git":{"createdTime":1749107714000,"updatedTime":1749119486000,"contributors":[{"name":"Paper-Dragon","username":"Paper-Dragon","email":"2678885646@qq.com","commits":2,"url":"https://github.com/Paper-Dragon"}],"changelog":[{"hash":"579a417243fec121aa68684bc59eed27fa2f3d59","time":1749119486000,"email":"2678885646@qq.com","author":"Paper-Dragon","message":"1.2"},{"hash":"bb8a1e70db02e492932b7b6fa6bc7e5ab0190842","time":1749107714000,"email":"2678885646@qq.com","author":"Paper-Dragon","message":"src/note-book/AI-Training/大语言模型基础.md"}]},"readingTime":{"minutes":3.64,"words":1091},"filePathRelative":"note-book/AI-Training/大语言模型基础.md","excerpt":"\\n<h2>大语言模型概念</h2>\\n<p>定义： 通常指具有超大规模参数的预训练模型</p>\\n<p>架构：主要为transformer解码器架构</p>\\n<p>训练：</p>\\n<ul>\\n<li>预训练（base model)  建立模型的<strong>基础能力</strong>\\n<ul>\\n<li>数据： 海量文本数据</li>\\n<li>优化：预测下一个词</li>\\n</ul>\\n</li>\\n<li>后训练 (instruct model) 增强模型的<strong>任务能力</strong>\\n<ul>\\n<li>数据： 大量指令数据</li>\\n<li>优化： SFT、RL等方法</li>\\n</ul>\\n</li>\\n<li>下游应用\\n<ul>\\n<li>测速（推理）</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{x as comp,E as data};
