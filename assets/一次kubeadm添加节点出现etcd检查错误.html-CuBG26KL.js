import{aq as s,ar as n,aw as a,av as i}from"./app-B6pX3YiM.js";const t={};function d(l,e){return i(),n("div",null,e[0]||(e[0]=[a(`<h1 id="一次kubeadm添加节点出现etcd检查错误" tabindex="-1"><a class="header-anchor" href="#一次kubeadm添加节点出现etcd检查错误"><span>一次kubeadm添加节点出现etcd检查错误</span></a></h1><blockquote><p>错误关键字</p><p>执行 kubeadm join... 时</p><p>[check-etcd] Checking that the etcd cluster is healthy</p><p>error execution phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://10.8.18.105:2379 with maintenance client: context deadline exceeded To see the stack trace of this error execute with --v=5 or higher</p></blockquote><h2 id="一-问题描述" tabindex="-1"><a class="header-anchor" href="#一-问题描述"><span>一 问题描述</span></a></h2><p>Kubernetes 集群中总共有三台 Master，分别是：</p><ul><li>k8s-master01</li><li>k8s-master02</li><li>k8s-master03</li></ul><p>对其中 <code>k8s-master02</code> Master 节点服务器进行了内核和软件升级操作，从而先将其暂时剔出集群，然后进行升级，完成后准备重新加入到 Kubernetes 集群，通过 Kubeadm 执行，输入下面命令：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>kubeadm join k8s-lb:16443 --token j7p6dr.zx0rh80lqn8unpty     --discovery-token-ca-cert-hash sha256:41a1353a03c99f46868294c28f9948bbc2cca957d98eb010435a493112ec7caa     --control-plane --certificate-key 5990f26f91d034a464692c13b31160d6d20df54fd8e3988d560e315c6ddb61aa</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>在执行过程中，输出下面日志，提示 etcd 监控检查失败：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>......</span></span>
<span class="line"><span>[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span></span>
<span class="line"><span>W0329 00:01:51.364121   19209 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span></span>
<span class="line"><span>[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span></span>
<span class="line"><span>W0329 00:01:51.373807   19209 manifests.go:214] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span></span>
<span class="line"><span>[check-etcd] Checking that the etcd cluster is healthy</span></span>
<span class="line"><span></span></span>
<span class="line"><span>error execution phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://10.8.18.105:2379 </span></span>
<span class="line"><span>with maintenance client: context deadline exceeded</span></span>
<span class="line"><span>To see the stack trace of this error execute with --v=5 or higher</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>根据关键信息 <code>&quot;error execution phase check-etcd&quot;</code> 可知，可能是在执行加入 <code>etcd</code> 时候出现的错误，导致 <code>master</code> 无法加入原先的 <code>kubernetes</code> 集群。</p><h2 id="二-分析问题" tabindex="-1"><a class="header-anchor" href="#二-分析问题"><span>二 分析问题</span></a></h2><h3 id="_1-查看集群节点列表" tabindex="-1"><a class="header-anchor" href="#_1-查看集群节点列表"><span>1 查看集群节点列表</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[root@k8s-master01 ~]# kubectl get nodes</span></span>
<span class="line"><span>NAME           STATUS   ROLES    AGE     VERSION</span></span>
<span class="line"><span>k8s-master01   Ready    master   4d20h   v1.18.2</span></span>
<span class="line"><span>k8s-master03   Ready    master   4d20h   v1.18.2</span></span>
<span class="line"><span>k8s-node01     Ready    worker   4d18h   v1.18.2</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可以看到，<code>k8s-master02</code> 节点确实不在节点列表中</p><h3 id="_2-查看-kubeadm-配置信息" tabindex="-1"><a class="header-anchor" href="#_2-查看-kubeadm-配置信息"><span>2 查看 Kubeadm 配置信息</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>ClusterStatus:</span></span>
<span class="line"><span>----</span></span>
<span class="line"><span>apiEndpoints:</span></span>
<span class="line"><span>  k8s-master01:</span></span>
<span class="line"><span>    advertiseAddress: 172.20.5.11</span></span>
<span class="line"><span>    bindPort: 6443</span></span>
<span class="line"><span>  k8s-master02:</span></span>
<span class="line"><span>    advertiseAddress: 172.20.5.12</span></span>
<span class="line"><span>    bindPort: 6443</span></span>
<span class="line"><span>  k8s-master03:</span></span>
<span class="line"><span>    advertiseAddress: 172.20.5.13</span></span>
<span class="line"><span>    bindPort: 6443</span></span>
<span class="line"><span>apiVersion: kubeadm.k8s.io/v1beta2</span></span>
<span class="line"><span>kind: ClusterStatus</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可也看到 <code>k8s-master02</code> 节点信息还存在与 <code>kubeadm</code> 配置中，说明 <code>etcd</code> 中还存储着 <code>k8s-master02</code> 相关信息。</p><h3 id="_3-分析问题所在及解决方案" tabindex="-1"><a class="header-anchor" href="#_3-分析问题所在及解决方案"><span>3 分析问题所在及解决方案</span></a></h3><p>因为集群是通过 <code>kubeadm</code> 工具搭建的，且使用了 <code>etcd</code> 镜像方式与 <code>master</code> 节点一起，所以每个 <code>Master</code> 节点上都会存在一个 <code>etcd</code> 容器实例。当剔除一个 <code>master</code> 节点时 <code>etcd</code> 集群未删除剔除的节点的 <code>etcd</code> 成员信息，该信息还存在 <code>etcd</code> 集群列表中。</p><p>所以，我们需要 <strong>进入 etcd 手动删除 etcd 成员信息</strong>。</p><h2 id="三-解决问题" tabindex="-1"><a class="header-anchor" href="#三-解决问题"><span>三 解决问题</span></a></h2><h3 id="_1-获取-etcd-镜像列表" tabindex="-1"><a class="header-anchor" href="#_1-获取-etcd-镜像列表"><span>1 获取 Etcd 镜像列表</span></a></h3><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[root@k8s-master01 ~]# kubectl get pods -n kube-system | grep etcd</span></span>
<span class="line"><span>etcd-k8s-master01                         1/1     Running            4          4d20h</span></span>
<span class="line"><span>etcd-k8s-master03                         1/1     Running            1          4d20h</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-进入-etcd-容器并删除节点信息" tabindex="-1"><a class="header-anchor" href="#_2-进入-etcd-容器并删除节点信息"><span>2 进入 Etcd 容器并删除节点信息</span></a></h3><p>选择上面两个 etcd 中任意一个 pod，通过 kubectl 工具进入 pod 内部</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[root@k8s-master01 ~]# kubectl exec -it -n kube-system etcd-k8s-master01 sh</span></span>
<span class="line"><span>kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>进入容器后，按下面步执行</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>## 配置环境</span></span>
<span class="line"><span># export ETCDCTL_API=3</span></span>
<span class="line"><span># alias etcdctl=&#39;etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key&#39;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>## 查看 etcd 集群成员列表</span></span>
<span class="line"><span># etcdctl member list</span></span>
<span class="line"><span>a9b6a1341829d62a, started, k8s-master03, https://172.20.5.13:2380, https://172.20.5.13:2379, false</span></span>
<span class="line"><span>d1c737a26ea4dd70, started, k8s-master01, https://172.20.5.11:2380, https://172.20.5.11:2379, false</span></span>
<span class="line"><span>fe2d4a2a33304913, started, k8s-master02, https://172.20.5.12:2380, https://172.20.5.12:2379, false</span></span>
<span class="line"><span></span></span>
<span class="line"><span>## 删除 etcd 集群成员 k8s-master02</span></span>
<span class="line"><span># etcdctl member remove fe2d4a2a33304913</span></span>
<span class="line"><span>Member fe2d4a2a33304913 removed from cluster 36067d1f1ca3f1db</span></span>
<span class="line"><span></span></span>
<span class="line"><span>## 退出容器</span></span>
<span class="line"><span># exit</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_3-再次尝试加入集群" tabindex="-1"><a class="header-anchor" href="#_3-再次尝试加入集群"><span>3 再次尝试加入集群</span></a></h3><p>通过 <code>kubeadm</code> 命令再次尝试将 <code>k8s-master02</code> 节点加入集群，在执行前首先进入到 <code>k8s-master02</code> 节点服务器，执行 <code>kubeadm</code> 的清除命令：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>$ kubeadm reset</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>然后尝试加入 kubernetes 集群：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" data-title="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[check-etcd] Checking that the etcd cluster is healthy</span></span>
<span class="line"><span>[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.18&quot; ConfigMap in the kube-system namespace</span></span>
<span class="line"><span>[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span></span>
<span class="line"><span>[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span></span>
<span class="line"><span>[kubelet-start] Starting the kubelet</span></span>
<span class="line"><span>[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span></span>
<span class="line"><span>[etcd] Announced new etcd member joining to the existing etcd cluster</span></span>
<span class="line"><span>[etcd] Creating static Pod manifest for &quot;etcd&quot;</span></span>
<span class="line"><span>[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s</span></span>
<span class="line"><span>{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2020-12-22T11:26:23.560+0800&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:61&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;passthrough:///https://172.20.5.12:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;}</span></span>
<span class="line"><span>[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span></span>
<span class="line"><span>[mark-control-plane] Marking the node k8s-master02 as control-plane by adding the label &quot;node-role.kubernetes.io/master=&#39;&#39;&quot;</span></span>
<span class="line"><span>[mark-control-plane] Marking the node k8s-master02 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>This node has joined the cluster and a new control plane instance was created:</span></span>
<span class="line"><span></span></span>
<span class="line"><span>* Certificate signing request was sent to apiserver and approval was received.</span></span>
<span class="line"><span>* The Kubelet was informed of the new secure connection details.</span></span>
<span class="line"><span>* Control plane (master) label and taint were applied to the new node.</span></span>
<span class="line"><span>* The Kubernetes control plane instances scaled up.</span></span>
<span class="line"><span>* A new etcd member was added to the local/stacked etcd cluster.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>To start administering your cluster from this node, you need to run the following as a regular user:</span></span>
<span class="line"><span></span></span>
<span class="line"><span>        mkdir -p $HOME/.kube</span></span>
<span class="line"><span>        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span></span>
<span class="line"><span>        sudo chown $(id -u):$(id -g) $HOME/.kube/config</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Run &#39;kubectl get nodes&#39; to see this node join the cluster.</span></span>
<span class="line"><span>[root@k8s-master01 ~]# kubectl get nodes</span></span>
<span class="line"><span>NAME           STATUS   ROLES    AGE     VERSION</span></span>
<span class="line"><span>k8s-master01   Ready    master   4d20h   v1.18.2</span></span>
<span class="line"><span>k8s-master02   Ready    master   7m38s   v1.18.2</span></span>
<span class="line"><span>k8s-master03   Ready    master   4d20h   v1.18.2</span></span>
<span class="line"><span>k8s-node01     Ready    worker   4d18h   v1.18.2</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,33)]))}const r=s(t,[["render",d],["__file","一次kubeadm添加节点出现etcd检查错误.html.vue"]]),p=JSON.parse('{"path":"/note-book/Kubernetes/%E4%B8%80%E6%AC%A1kubeadm%E6%B7%BB%E5%8A%A0%E8%8A%82%E7%82%B9%E5%87%BA%E7%8E%B0etcd%E6%A3%80%E6%9F%A5%E9%94%99%E8%AF%AF.html","title":"一次kubeadm添加节点出现etcd检查错误","lang":"zh-CN","frontmatter":{"description":"一次kubeadm添加节点出现etcd检查错误 错误关键字 执行 kubeadm join... 时 [check-etcd] Checking that the etcd cluster is healthy error execution phase check-etcd: etcd cluster is not healthy: failed t...","head":[["meta",{"property":"og:url","content":"https://www.geekery.cn/note-book/Kubernetes/%E4%B8%80%E6%AC%A1kubeadm%E6%B7%BB%E5%8A%A0%E8%8A%82%E7%82%B9%E5%87%BA%E7%8E%B0etcd%E6%A3%80%E6%9F%A5%E9%94%99%E8%AF%AF.html"}],["meta",{"property":"og:site_name","content":"运维开发绿皮书"}],["meta",{"property":"og:title","content":"一次kubeadm添加节点出现etcd检查错误"}],["meta",{"property":"og:description","content":"一次kubeadm添加节点出现etcd检查错误 错误关键字 执行 kubeadm join... 时 [check-etcd] Checking that the etcd cluster is healthy error execution phase check-etcd: etcd cluster is not healthy: failed t..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-08-13T15:08:38.000Z"}],["meta",{"property":"article:modified_time","content":"2023-08-13T15:08:38.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"一次kubeadm添加节点出现etcd检查错误\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2023-08-13T15:08:38.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"PaperDragon\\",\\"url\\":\\"https://github.com/Paper-Dragon\\",\\"email\\":\\"2678885646@qq.com\\"}]}"]]},"git":{"createdTime":1691939318000,"updatedTime":1691939318000,"contributors":[{"name":"PaperDragon","username":"PaperDragon","email":"2678885646@qq.com","commits":1,"url":"https://github.com/PaperDragon"}]},"readingTime":{"minutes":3.92,"words":1175},"filePathRelative":"note-book/Kubernetes/一次kubeadm添加节点出现etcd检查错误.md","localizedDate":"2023年8月13日","excerpt":"\\n<blockquote>\\n<p>错误关键字</p>\\n<p>执行 kubeadm join... 时</p>\\n<p>[check-etcd] Checking that the etcd cluster is healthy</p>\\n<p>error execution phase check-etcd: etcd cluster is not healthy: failed to dial endpoint https://10.8.18.105:2379 with maintenance client: context deadline exceeded To see the stack trace of this error execute with --v=5 or higher</p>\\n</blockquote>","autoDesc":true}');export{r as comp,p as data};
